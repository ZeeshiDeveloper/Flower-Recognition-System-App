{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c23834a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ec38063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fb4bd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23213 images belonging to 23 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        'training_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b108442f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4242 images belonging to 23 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        'test_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd48ed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30cd3c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=64 , kernel_size=3 , activation='relu' , input_shape=[64,64,3]))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfdad454",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=64 , kernel_size=3 , activation='relu' ))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2 , strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b152e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c58f3d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f87380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b0ed327",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=23 , activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3151d607",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'rmsprop' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02567ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "726/726 [==============================] - 205s 283ms/step - loss: 1.2481 - accuracy: 0.6058 - val_loss: 1.2861 - val_accuracy: 0.5839\n",
      "Epoch 2/60\n",
      "726/726 [==============================] - 236s 325ms/step - loss: 1.2488 - accuracy: 0.6132 - val_loss: 1.2388 - val_accuracy: 0.5908\n",
      "Epoch 3/60\n",
      "726/726 [==============================] - 277s 382ms/step - loss: 1.2238 - accuracy: 0.6176 - val_loss: 1.2299 - val_accuracy: 0.6009\n",
      "Epoch 4/60\n",
      "726/726 [==============================] - 259s 357ms/step - loss: 1.2191 - accuracy: 0.6170 - val_loss: 1.2011 - val_accuracy: 0.6256\n",
      "Epoch 5/60\n",
      "726/726 [==============================] - 224s 308ms/step - loss: 1.2104 - accuracy: 0.6245 - val_loss: 1.1874 - val_accuracy: 0.6301\n",
      "Epoch 6/60\n",
      "726/726 [==============================] - 199s 275ms/step - loss: 1.1990 - accuracy: 0.6330 - val_loss: 1.2120 - val_accuracy: 0.6190\n",
      "Epoch 7/60\n",
      "726/726 [==============================] - 199s 274ms/step - loss: 1.1878 - accuracy: 0.6357 - val_loss: 1.2181 - val_accuracy: 0.6132\n",
      "Epoch 8/60\n",
      "726/726 [==============================] - 198s 272ms/step - loss: 1.1722 - accuracy: 0.6408 - val_loss: 1.2021 - val_accuracy: 0.6200\n",
      "Epoch 9/60\n",
      "726/726 [==============================] - 227s 312ms/step - loss: 1.1638 - accuracy: 0.6402 - val_loss: 1.1116 - val_accuracy: 0.6561\n",
      "Epoch 10/60\n",
      "726/726 [==============================] - 248s 342ms/step - loss: 1.1468 - accuracy: 0.6497 - val_loss: 1.2590 - val_accuracy: 0.6226\n",
      "Epoch 11/60\n",
      "726/726 [==============================] - 264s 364ms/step - loss: 1.1414 - accuracy: 0.6506 - val_loss: 1.0937 - val_accuracy: 0.6629\n",
      "Epoch 12/60\n",
      "726/726 [==============================] - 224s 309ms/step - loss: 1.1357 - accuracy: 0.6566 - val_loss: 1.0939 - val_accuracy: 0.6596\n",
      "Epoch 13/60\n",
      "726/726 [==============================] - 208s 287ms/step - loss: 1.1165 - accuracy: 0.6608 - val_loss: 1.1041 - val_accuracy: 0.6620\n",
      "Epoch 14/60\n",
      "726/726 [==============================] - 208s 287ms/step - loss: 1.1183 - accuracy: 0.6600 - val_loss: 1.0981 - val_accuracy: 0.6723\n",
      "Epoch 15/60\n",
      "726/726 [==============================] - 206s 284ms/step - loss: 1.1083 - accuracy: 0.6658 - val_loss: 1.0687 - val_accuracy: 0.6752\n",
      "Epoch 16/60\n",
      "726/726 [==============================] - 205s 283ms/step - loss: 1.0994 - accuracy: 0.6696 - val_loss: 1.1292 - val_accuracy: 0.6561\n",
      "Epoch 17/60\n",
      "726/726 [==============================] - 207s 285ms/step - loss: 1.0805 - accuracy: 0.6772 - val_loss: 1.1015 - val_accuracy: 0.6629\n",
      "Epoch 18/60\n",
      "726/726 [==============================] - ETA: 0s - loss: 1.0741 - accuracy: 0.6786"
     ]
    }
   ],
   "source": [
    "cnn.fit(x = training_set , validation_data = test_set , epochs = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "220fc161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 143ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Aster': 0,\n",
       " 'Bellflower': 1,\n",
       " 'Iris': 2,\n",
       " 'Lavender': 3,\n",
       " 'Lily': 4,\n",
       " 'Marigold': 5,\n",
       " 'Orchid': 6,\n",
       " 'Poppy': 7,\n",
       " 'airplane': 8,\n",
       " 'apple': 9,\n",
       " 'black_eyed_susan': 10,\n",
       " 'car': 11,\n",
       " 'cat': 12,\n",
       " 'daisy': 13,\n",
       " 'dandelion': 14,\n",
       " 'dog': 15,\n",
       " 'fruit': 16,\n",
       " 'motorbike': 17,\n",
       " 'person': 18,\n",
       " 'rose': 19,\n",
       " 'sunflower': 20,\n",
       " 'tulip': 21,\n",
       " 'water_lily': 22}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "test_image = image.load_img('Prediction/das.webp',target_size=(64,64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image,axis=0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c12e33d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0486778  0.01995941 0.05284692 0.0382424  0.05538775 0.07003854\n",
      "  0.08590579 0.08046304 0.02042519 0.01943046 0.03773413 0.03356024\n",
      "  0.02304955 0.04336585 0.04226495 0.02193452 0.02318351 0.03124623\n",
      "  0.0305454  0.03534765 0.06770802 0.07618597 0.04249658]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e47236",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
